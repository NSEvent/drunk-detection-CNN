{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# e.g.: python drunk_detector data --train-files data/train/*/* --test-files data/test/*/* --val-files data/validation/*/*\n",
    "#       python drunk_detector train -d data_2020-04-09_12-20-39.pickle\n",
    "#       python drunk_detector predict -d data_2020-04-09_12-20-39.pickle -c voter_2020-04-10_20-51-32.pickle\n",
    "# tensorboard --logdir=\"logs/\"\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from PIL import Image, ImageOps\n",
    "import re\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "import pandas as pd\n",
    "\n",
    "# Model types\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Tensorflow Keras imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model, save_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "\n",
    "class DrunkDetector:\n",
    "    def __init__(self, args):\n",
    "        self.face_filename_pattern = '[0-4][0-9]_[a-z]*_[0-4]_f_[FM]_[0-9_]*\\.tif'\n",
    "        self.sober_filename_patter = '[0-4][0-9]_[a-z]*_1_f_[FM]_[0-9_]*\\.tif'\n",
    "        self.args = args\n",
    "        self.data = { 'train': [], 'test': [], 'val': [] }\n",
    "\n",
    "    def format_data(self):\n",
    "        self.read_images(self.args.train_files, 'train')\n",
    "        self.read_images(self.args.test_files, 'test')\n",
    "        self.read_images(self.args.val_files, 'val')\n",
    "#         self.augment_data('train')\n",
    "#         self.augment_data('val')\n",
    "#         self.augment_data('test')\n",
    "        # Current data shows that self.augment_data() produces data not suitable for training on CNN\n",
    "        # Using flip for data augmentation in self.train()\n",
    "        curr_datetime = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "        filename = '{}_{}.pickle'.format('data', curr_datetime)\n",
    "        with open(os.path.join(self.args.output_dir, filename), 'wb') as out_file:\n",
    "            pickle.dump(self.data, out_file)\n",
    "\n",
    "    def augment_data(self, split_set):\n",
    "\n",
    "        augmented_data = []\n",
    "        for datum in self.data[split_set]:\n",
    "            for _ in range(1 if datum['y'] else 1):\n",
    "                # horizontal flip with a noise on non-zero values\n",
    "                variance = [[[20 if col else 0 for col in row] for row in layer ] for layer in datum['thermal_frames']]\n",
    "                thermal_frames = np.random.normal(np.flip(datum['thermal_frames'], 2), variance)\n",
    "                thermal_sum = np.zeros((128, 160))\n",
    "                for f, frame in enumerate(thermal_frames):\n",
    "                    min_val = max(np.amin(frame), 0)\n",
    "                    frame -= min_val\n",
    "                    for r, row in enumerate(frame):\n",
    "                        for c, val in enumerate(row):\n",
    "                            if val < 0:\n",
    "                                val = 0\n",
    "                                thermal_frames[f, r, c] = 0\n",
    "                            thermal_sum[r, c] += val\n",
    "                augmented_data.append({\n",
    "                    'filename': datum['filename'] + '_flipped',\n",
    "                    'thermal_sum': thermal_sum,\n",
    "                    'thermal_frames': thermal_frames,\n",
    "                    'y': datum['y']\n",
    "                })\n",
    "\n",
    "                # noisy version of image\n",
    "                variance = [[[25 if col else 0 for col in row] for row in layer ] for layer in datum['thermal_frames']]\n",
    "                thermal_frames = np.random.normal(datum['thermal_frames'], variance)\n",
    "                thermal_sum = np.zeros((128, 160))\n",
    "                for f, frame in enumerate(thermal_frames):\n",
    "                    min_val = max(np.amin(frame), 0)\n",
    "                    frame -= min_val\n",
    "                    for r, row in enumerate(frame):\n",
    "                        for c, val in enumerate(row):\n",
    "                            if val < 0:\n",
    "                                val = 0\n",
    "                                thermal_frames[f, r, c] = 0\n",
    "                            thermal_sum[r, c] += val\n",
    "                augmented_data.append({\n",
    "                    'filename': datum['filename'] + '_noisy',\n",
    "                    'thermal_sum': thermal_sum,\n",
    "                    'thermal_frames': thermal_frames,\n",
    "                    'y': datum['y']\n",
    "                })\n",
    "\n",
    "                # blurred version of image\n",
    "                thermal_frames = gaussian_filter(datum['thermal_frames'], sigma=3)\n",
    "                thermal_sum = np.zeros((128, 160))\n",
    "                for _, frame in enumerate(thermal_frames):\n",
    "                    min_val = max(np.amin(frame), 0)\n",
    "                    frame -= min_val\n",
    "                    for r, row in enumerate(frame):\n",
    "                        for c, val in enumerate(row):\n",
    "                            thermal_sum[r, c] += val\n",
    "                augmented_data.append({\n",
    "                    'filename': datum['filename'] + '_blur',\n",
    "                    'thermal_sum': thermal_sum,\n",
    "                    'thermal_frames': thermal_frames,\n",
    "                    'y': datum['y']\n",
    "                })\n",
    "\n",
    "            # im = Image.fromarray(thermal_sum/np.max(thermal_sum) * 255)\n",
    "            # im.show()\n",
    "            # im_orig = Image.fromarray(datum['thermal_sum']/np.max(datum['thermal_sum']) * 255)\n",
    "            # im_orig.show()\n",
    "            # im_orig.show()\n",
    "\n",
    "        self.data[split_set] += augmented_data\n",
    "\n",
    "    def read_images(self, files, split_set):\n",
    "        for filename in files:\n",
    "            basename = os.path.basename(filename)\n",
    "            if not self.is_face_image(basename):\n",
    "                continue\n",
    "\n",
    "            with Image.open(filename) as img:\n",
    "                self.data[split_set].append({\n",
    "                    'filename': basename,\n",
    "                    'thermal_sum': np.zeros((128, 160)),\n",
    "                    'thermal_frames': np.zeros((img.n_frames, 128, 160)),\n",
    "                    'y': self.is_sober_image(basename)\n",
    "                })\n",
    "\n",
    "                for i in range(img.n_frames):\n",
    "                    img.seek(i)\n",
    "                    frame_data = np.array(img)\n",
    "                    min_val = np.amin(frame_data)\n",
    "                    frame_data -= min_val\n",
    "                    for j in range(img.height):\n",
    "                        for k in range(img.width):\n",
    "                            self.data[split_set][-1]['thermal_sum'][j, k] += frame_data[j, k]\n",
    "\n",
    "                    self.data[split_set][-1]['thermal_frames'][i] = frame_data\n",
    "\n",
    "    def is_face_image(self, filename):\n",
    "        return re.match(self.face_filename_pattern, filename) is not None\n",
    "\n",
    "    def is_sober_image(self, filename):\n",
    "        if re.match(self.sober_filename_patter, filename) is not None:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "        with open(self.args.classifier, 'rb') as clf_file:\n",
    "            self.clf = pickle.load(clf_file)\n",
    "\n",
    "        with open(self.args.data, 'rb') as data_file:\n",
    "            self.data = pickle.load(data_file)\n",
    "\n",
    "        if self.args.data_mode == 'frames':\n",
    "            self.test_X = np.array([datum['thermal_frames'] for datum in self.data['test']])\n",
    "        elif self.args.data_mode == 'sum':\n",
    "            self.test_X = np.array([datum['thermal_sum'] for datum in self.data['test']])\n",
    "\n",
    "        self.test_X_2d = np.array([datum.flatten() for datum in self.test_X])\n",
    "        self.test_y = np.array([datum['y'] for datum in self.data['test']])\n",
    "\n",
    "        pred_y =  self.clf.predict(self.test_X_2d)\n",
    "        print('Classifier accuracy:', accuracy_score(self.test_y, pred_y))\n",
    "\n",
    "    def train(self):\n",
    "        if self.args.data is None:\n",
    "            print('Error: No data given.')\n",
    "            exit(1)\n",
    "\n",
    "        with open(self.args.data, 'rb') as data_file:\n",
    "            self.data = pickle.load(data_file)\n",
    "\n",
    "        if self.args.data_mode == 'frames':\n",
    "            self.train_X = np.array([datum['thermal_frames'] for datum in self.data['train']])\n",
    "            self.val_X = np.array([datum['thermal_frames'] for datum in self.data['val']])\n",
    "            self.test_X = np.array([datum['thermal_frames'] for datum in self.data['test']])\n",
    "        elif self.args.data_mode == 'sum':\n",
    "            self.train_X = np.array([datum['thermal_sum'] for datum in self.data['train']])\n",
    "            self.val_X = np.array([datum['thermal_sum'] for datum in self.data['val']])\n",
    "            self.test_X = np.array([datum['thermal_sum'] for datum in self.data['test']])\n",
    "\n",
    "        self.train_X_2d = np.array([datum.flatten() for datum in self.train_X])\n",
    "        self.val_X_2d = np.array([datum.flatten() for datum in self.val_X])\n",
    "        self.test_X_2d = np.array([datum.flatten() for datum in self.test_X])\n",
    "\n",
    "        self.train_y = np.array([datum['y'] for datum in self.data['train']])\n",
    "        self.val_y = np.array([datum['y'] for datum in self.data['val']])\n",
    "        self.test_y = np.array([datum['y'] for datum in self.data['test']])\n",
    "\n",
    "        # Flip data\n",
    "        flip_seq = iaa.Sequential([\n",
    "            iaa.Fliplr(1), # horizontally flip all of the images\n",
    "        ])\n",
    "        flip_data = flip_seq(images=self.train_X)\n",
    "        self.train_X = np.concatenate((self.train_X, flip_data), axis=0)\n",
    "        self.train_y = np.concatenate((self.train_y, self.train_y), axis=0)\n",
    "        flip_data = flip_seq(images=self.test_X)\n",
    "        self.test_X = np.concatenate((self.test_X, flip_data), axis=0)\n",
    "        self.test_y = np.concatenate((self.test_y, self.test_y), axis=0)\n",
    "        flip_data = flip_seq(images=self.val_X)\n",
    "        self.val_X = np.concatenate((self.val_X, flip_data), axis=0)\n",
    "        self.val_y = np.concatenate((self.val_y, self.val_y), axis=0)\n",
    "\n",
    "        # Shuffle data\n",
    "        self.train_X, self.train_y = shuffle_pair(self.train_X, self.train_y)\n",
    "        self.val_X, self.val_y = shuffle_pair(self.val_X, self.val_y)\n",
    "        self.test_X, self.test_y = shuffle_pair(self.test_X, self.test_y)\n",
    "\n",
    "        # svc = self.train_svm()\n",
    "        # rfc = self.train_rf()\n",
    "        # lr = self.train_lr()\n",
    "        # sgd = self.train_sgd()\n",
    "        # knn = self.train_knn()\n",
    "        # dt = self.train_dt()\n",
    "        # voter = self.train_voter()\n",
    "        # self.train_mlp()\n",
    "        # self.test_best_model()\n",
    "        self.train_cnn_hyperparameters()\n",
    "\n",
    "#         curr_datetime = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "#         filename = '{}_{}.pickle'.format(self.args.name, curr_datetime)\n",
    "#         with open(os.path.join(self.args.output_dir, filename), 'wb') as out_file:\n",
    "#             pickle.dump(voter, out_file)\n",
    "\n",
    "\n",
    "    # Voting Classifier\n",
    "    def train_voter(self):\n",
    "        dt = DecisionTreeClassifier()\n",
    "        knn = KNeighborsClassifier()\n",
    "        lr = LogisticRegression(max_iter=200)\n",
    "        rfc = RandomForestClassifier(class_weight='balanced', n_estimators=250, min_impurity_decrease=0)\n",
    "        sgd = SGDClassifier(loss='log')\n",
    "        svc = SVC(probability=True)\n",
    "        voter = VotingClassifier(\n",
    "            estimators=[('dt', dt), ('knn', knn), ('lr', lr), ('rfc', rfc), ('sgd', sgd), ('svc', svc)],\n",
    "            voting='soft')\n",
    "\n",
    "        voter.fit(self.train_X_2d, self.train_y)\n",
    "        pred_y = voter.predict(self.val_X_2d)\n",
    "        # print('voter accuracy:', accuracy_score(self.val_y, pred_y))\n",
    "        return voter\n",
    "\n",
    "    # Decision Tree\n",
    "    def train_dt(self):\n",
    "        dt = DecisionTreeClassifier()\n",
    "        dt.fit(self.train_X_2d, self.train_y)\n",
    "        # pred_y = dt.predict(self.val_X_2d)\n",
    "        # print('Decision Tree accuracy:', accuracy_score(self.val_y, pred_y))\n",
    "        return dt\n",
    "\n",
    "    # K Nearest Neighbors\n",
    "    def train_knn(self):\n",
    "        knn = KNeighborsClassifier()\n",
    "        knn.fit(self.train_X_2d, self.train_y)\n",
    "        # pred_y = knn.predict(self.val_X_2d)\n",
    "        # print('KNN accuracy:', accuracy_score(self.val_y, pred_y))\n",
    "        return knn\n",
    "\n",
    "    # Logistic Regression\n",
    "    def train_lr(self):\n",
    "        lr = LogisticRegression(max_iter=200)\n",
    "        lr.fit(self.train_X_2d, self.train_y)\n",
    "        # pred_y = lr.predict(self.val_X_2d)\n",
    "        # print('logistic regression accuracy:', accuracy_score(self.val_y, pred_y))\n",
    "        return lr\n",
    "\n",
    "    # Multi-Layer Perceptron\n",
    "    def train_mlp(self):\n",
    "        parameters = {\n",
    "            'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "            'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "            'solver': ['lbfgs', 'sgd', 'adam']\n",
    "        }\n",
    "        mlp = MLPClassifier()\n",
    "        clf = GridSearchCV(mlp, parameters)\n",
    "        clf.fit(self.train_X_2d, self.train_y)\n",
    "        pred_y = clf.predict(self.val_X_2d)\n",
    "        print('Multi-Layer Perceptron accuracy:', accuracy_score(self.val_y, pred_y))\n",
    "\n",
    "    # Random Forest\n",
    "    def train_rf(self):\n",
    "        parameters = {\n",
    "            'n_estimators': [50, 100, 250, 500], # 250\n",
    "            'min_impurity_decrease': [0, 0.25, 0.5], # 0\n",
    "            'class_weight': [None, 'balanced'] # balanced\n",
    "        }\n",
    "        rfc = RandomForestClassifier(class_weight='balanced', n_estimators=250, min_impurity_decrease=0)\n",
    "        # clf = GridSearchCV(rfc, parameters)\n",
    "        rfc.fit(self.train_X_2d, self.train_y)\n",
    "        # pred_y = rfc.predict(self.val_X_2d)\n",
    "        # print('random forest accuracy:', accuracy_score(self.val_y, pred_y))\n",
    "        # print('\\tparams:', clf.best_params_)\n",
    "        return rfc\n",
    "\n",
    "    # Stochastic Gradient Descent\n",
    "    def train_sgd(self):\n",
    "        sgd = SGDClassifier()\n",
    "        sgd.fit(self.train_X_2d, self.train_y)\n",
    "        # pred_y = sgd.predict(self.val_X_2d)\n",
    "        # print('Stochastic Gradient Descent accuracy:', accuracy_score(self.val_y, pred_y))\n",
    "        return sgd\n",
    "\n",
    "    # Support Vector Machine\n",
    "    def train_svm(self):\n",
    "        svc = SVC()\n",
    "        svc.fit(self.train_X_2d, self.train_y)\n",
    "\n",
    "        # pred_y = svc.predict(self.val_X_2d)\n",
    "        # print('svm accuracy:', accuracy_score(self.val_y, pred_y))\n",
    "        return svc\n",
    "\n",
    "\n",
    "    def prepare_data_cnn(self):\n",
    "        assert self.args.data_mode == 'sum', \\\n",
    "                'Use [-m sum] for training CNN'\n",
    "\n",
    "        self.train_X = tf.keras.utils.normalize(self.train_X, axis=1)\n",
    "        self.val_X = tf.keras.utils.normalize(self.val_X, axis=1)\n",
    "        self.test_X = tf.keras.utils.normalize(self.test_X, axis=1)\n",
    "\n",
    "    # Convolutional Neural Network\n",
    "    def train_cnn(self):\n",
    "        self.prepare_data_cnn()\n",
    "\n",
    "        curr_datetime = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "        x, y = self.train_X.shape[1:]\n",
    "\n",
    "        model = Sequential()\n",
    "        # Layer 1\n",
    "        model.add(Conv2D(8, (3, 3), input_shape=(x,y,1)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        # Layer 2\n",
    "        model.add(Conv2D(16, (3, 3)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        # Layer 3\n",
    "        model.add(Conv2D(32, (3, 3)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        # Dense layer\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        # Output layer\n",
    "        model.add(Dense(1))\n",
    "        model.add(Activation('sigmoid'))\n",
    "\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                        optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "        # Treat every sober image with same weight as 3 drunk images\n",
    "        # https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#class_weights\n",
    "        class_weight = {0: (1/4)/2, 1: (3/4)/2}\n",
    "\n",
    "        #### Below is adapted from class example of CNN\n",
    "        num_epochs = 1000\n",
    "\n",
    "        # Holds performance statistics across epochs\n",
    "        perf_time = np.zeros((num_epochs, 4))\n",
    "\n",
    "        # Set up figure\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(111)\n",
    "\n",
    "        best_val = [np.inf, 0]\n",
    "        for epoch in np.arange(0,num_epochs):\n",
    "            model.fit(cnn_data(self.train_X), np.array(self.train_y),\n",
    "                      batch_size=32,\n",
    "                      epochs=1,\n",
    "                      verbose=1,\n",
    "                      class_weight=class_weight,\n",
    "                      validation_data=(cnn_data(self.val_X), np.array(self.val_y)),\n",
    "                      shuffle=True,\n",
    "                      use_multiprocessing=True)\n",
    "            # Check the performance on train/test/val\n",
    "            # The model.evaluate function returns an array: [loss, accuracy]\n",
    "            val = model.evaluate(cnn_data(self.val_X), np.array(self.val_y))  # val = [val_loss, val_accuracy]\n",
    "            new = [model.evaluate(cnn_data(self.train_X), np.array(self.train_y))[1],\n",
    "                   val[0], val[1],\n",
    "                   model.evaluate(cnn_data(self.test_X), np.array(self.test_y))[1]]\n",
    "            perf_time[epoch,:]=new\n",
    "\n",
    "            # Visualize\n",
    "            plt.plot(np.arange(0,epoch+1),perf_time[0:epoch+1,0],'b', label='train')\n",
    "            plt.plot(np.arange(0,epoch+1),perf_time[0:epoch+1,2],'r', label='validation')\n",
    "            plt.plot(np.arange(0,epoch+1),perf_time[0:epoch+1,3],'g', label='test')\n",
    "            plt.legend(loc='upper left')\n",
    "            plt.show()\n",
    "\n",
    "            # Test if validation performance has improved (val_loss)\n",
    "            if val[0] >= best_val[0]:\n",
    "                best_val[1] += 1\n",
    "            # Model improved\n",
    "            else:\n",
    "                best_val = [val[0], 0]\n",
    "                # Save current best model\n",
    "                filename_model = '{}_{}_epoch={}_val_acc={}.hdf5'.format('cnn_model', curr_datetime, epoch, val[1])\n",
    "                save_model(model, filename_model)\n",
    "            print (\"epoch %d, loss %f, number %d\" %(epoch, best_val[0], best_val[1]))\n",
    "\n",
    "            # Stop training if performance hasn't increased in STOP_ITERATIONS\n",
    "            STOP_ITERATIONS = 30\n",
    "            if best_val[1] > STOP_ITERATIONS:\n",
    "                break\n",
    "\n",
    "        # Export model and test/val/train plot\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.plot(np.arange(0,epoch+1),perf_time[0:epoch+1,0],'b', label='train')\n",
    "        plt.plot(np.arange(0,epoch+1),perf_time[0:epoch+1,2],'r', label='validation')\n",
    "        plt.plot(np.arange(0,epoch+1),perf_time[0:epoch+1,3],'g', label='test')\n",
    "        plt.legend(loc='upper left')\n",
    "\n",
    "        filename_fig = '{}_{}.png'.format('cnn_fig', curr_datetime)\n",
    "        plt.savefig(filename_fig)\n",
    "        plt.close('all') # Close fig to save memory\n",
    "\n",
    "        filename_model = '{}_{}.hdf5'.format('cnn_model', curr_datetime)\n",
    "        save_model(model, filename_model)\n",
    "\n",
    "        pred_y = model.predict_classes(cnn_data(self.test_X))\n",
    "        print('CNN accuracy:', accuracy_score(self.test_y, pred_y))\n",
    "        print('CNN confusion matrix\\n', confusion_matrix(self.test_y, pred_y))\n",
    "\n",
    "\n",
    "\n",
    "    # Determine if a prediction is statistically significantly better than predicting all drunk\n",
    "    def test_model_significance(self, pred_y):\n",
    "        subject = [] # Keep track of person\n",
    "        correct = [] # Keep track of images correctly predicted\n",
    "        model_name = [] # Keep track of corresponding model name\n",
    "\n",
    "        for i, (true, pred) in enumerate(zip(self.test_y, pred_y)):\n",
    "            subject.append(i)\n",
    "            correct.append(int(true==pred))\n",
    "            model_name.append('best')\n",
    "        pred_drunk = []\n",
    "        for _ in range(len(self.test_y)):\n",
    "            pred_drunk.append(1)\n",
    "        for i, (true, pred) in enumerate(zip(self.test_y, pred_drunk)):\n",
    "            subject.append(i)\n",
    "            correct.append(int(true==pred))\n",
    "            model_name.append('drunk')\n",
    "\n",
    "        anova_dict = {'Correct/Incorrect':correct,'Test_ID':subject,'Model_Name':model_name}\n",
    "        anova_df = pd.DataFrame(anova_dict)\n",
    "\n",
    "        anovarm = AnovaRM(data=anova_df, depvar='Correct/Incorrect', subject='Test_ID', within=['Model_Name'])\n",
    "        fit = anovarm.fit()\n",
    "        print(fit.summary())\n",
    "\n",
    "        \n",
    "    def demo_model(self, pred_y):\n",
    "        print('Making predictions on 70 images in test set')\n",
    "        \n",
    "        state = ['sober', 'drunk']\n",
    "        for i,(actual,pred) in enumerate(zip(self.test_y ,pred_y)):\n",
    "            if i > 0:\n",
    "                print(); print()\n",
    "\n",
    "            bw = Image.fromarray(self.test_X[i]*1500).convert('L')\n",
    "            add_border\n",
    "            display(add_border(bw, 25, int(actual)==int(pred)))\n",
    "            print(f'Actual: {state[int(actual)]}, Predicted: {state[int(pred)]}', flush=True)\n",
    "            input('Enter for next prediction:')\n",
    "\n",
    "            \n",
    "    def test_best_model(self):\n",
    "        self.prepare_data_cnn()\n",
    "\n",
    "        model = load_model('../models/n_clayers=1,clayer_sz=8,n_dlayers=0,dlayer_sz=512,lr=0.01,bat_size=32FINAL-test_acc=0.8714285492897034.hdf5')\n",
    "        pred_y = model.predict_classes(cnn_data(self.test_X))\n",
    "        print('CNN accuracy:', accuracy_score(self.test_y, pred_y))\n",
    "        print('CNN confusion matrix\\n', confusion_matrix(self.test_y, pred_y))\n",
    "        self.test_model_significance(pred_y)\n",
    "\n",
    "        plot_confusion_matrix(confusion_matrix(self.test_y, pred_y), target_names=['sober', 'drunk'])\n",
    "        import time; time.sleep(1)\n",
    "        self.demo_model(pred_y)\n",
    "\n",
    "\n",
    "\n",
    "    # Convolutional Neural Network hyperparameter tuning\n",
    "    def train_cnn_hyperparameters(self):\n",
    "        self.prepare_data_cnn()\n",
    "\n",
    "        curr_datetime = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "        x, y = self.train_X.shape[1:]\n",
    "\n",
    "\n",
    "        # saved_acc = pickle.load(open('saved_acc.pickle, 'rb'))\n",
    "\n",
    "        # hyperparameters\n",
    "        num_convlayers = [1, 2, 3]\n",
    "        convlayer_size = [8, 16, 32]\n",
    "        num_denselayers = [0, 1, 2]\n",
    "        denselayer_size = [128, 256, 512]\n",
    "        learning_rate = [0.01, 0.001, 0.0001]\n",
    "        batch_size = [16, 32]\n",
    "\n",
    "        # creates a list of all combinations of hyperparameters\n",
    "        param_grid = list(itertools.product(\n",
    "            num_convlayers, convlayer_size,\n",
    "            num_denselayers, denselayer_size,\n",
    "            learning_rate,\n",
    "            batch_size))\n",
    "\n",
    "        print(f'CNN hyperparameter tuning with {len(param_grid)} combinations')\n",
    "\n",
    "        # offset = len(saved_acc)\n",
    "        for i, params in enumerate(shuffle(param_grid)):\n",
    "\n",
    "            print(f'Run {i}/{len(param_grid)}')\n",
    "\n",
    "            # tuning parameters\n",
    "            num_convlayers, \\\n",
    "            convlayer_size, \\\n",
    "            num_denselayers, \\\n",
    "            denselayer_size, \\\n",
    "            learning_rate, \\\n",
    "            batch_size = params\n",
    "\n",
    "            NAME = f'n_clayers={num_convlayers},clayer_sz={convlayer_size},n_dlayers={num_denselayers},dlayer_sz={denselayer_size},lr={learning_rate},bat_size={batch_size}'\n",
    "            tensorboard = TensorBoard(log_dir=f'logs/{NAME}')\n",
    "\n",
    "#             save_best_model_loss = ModelCheckpoint(NAME + \"val_loss={val_loss:.4f}.hdf5\",\n",
    "#                                                   monitor='val_loss',\n",
    "#                                                   verbose=0, save_best_only=True,\n",
    "#                                                   save_weights_only=False,\n",
    "#                                                   mode='auto', save_freq='epoch')\n",
    "#             save_best_model_acc = tf.keras.callbacks.ModelCheckpoint(NAME + \"val_acc={val_accuracy:.4f}.hdf5\",\n",
    "#                                                      monitor='val_accuracy',\n",
    "#                                                      verbose=0, save_best_only=True,\n",
    "#                                                      save_weights_only=False,\n",
    "#                                                      mode='auto', save_freq='epoch')\n",
    "            early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "                                                    monitor='val_accuracy',\n",
    "                                                    verbose=1,\n",
    "                                                    patience=30,\n",
    "                                                    mode='auto',\n",
    "                                                    restore_best_weights=False)\n",
    "\n",
    "            model = Sequential()\n",
    "            # Conv layer 1\n",
    "            model.add(Conv2D(convlayer_size, (3, 3), input_shape=(x,y,1)))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "            # Additional Conv layers\n",
    "            for _ in range(num_convlayers-1):\n",
    "                model.add(Conv2D(convlayer_size, (3, 3)))\n",
    "                model.add(Activation('relu'))\n",
    "                model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "            # Dense layer 1\n",
    "            model.add(Flatten())\n",
    "            for _ in range(num_denselayers):\n",
    "                model.add(Dense(denselayer_size, activation='relu'))\n",
    "\n",
    "            if False:\n",
    "                model.add(BatchNormalization())\n",
    "\n",
    "            model.add(Dense(1))\n",
    "            model.add(Activation('sigmoid'))\n",
    "\n",
    "            model.compile(loss='binary_crossentropy',\n",
    "                            optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "                            metrics=['accuracy'])\n",
    "\n",
    "            # Treat every sober image with same weight as 3 drunk images\n",
    "            # https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#class_weights\n",
    "            class_weight = {0: (1/4)/2, 1: (3/4)/2}\n",
    "\n",
    "            num_epochs = 1000\n",
    "            model.fit(cnn_data(self.train_X), np.array(self.train_y),\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=num_epochs,\n",
    "                      verbose=1,\n",
    "                      class_weight=class_weight,\n",
    "                      validation_data=(cnn_data(self.val_X), np.array(self.val_y)),\n",
    "                      shuffle=True,\n",
    "                      use_multiprocessing=True,\n",
    "                      callbacks=[tensorboard, early_stopping])\n",
    "\n",
    "            save_model(model, NAME + f'FINAL-test_acc={model.evaluate(cnn_data(self.test_X),self.test_y)[1]}.hdf5')\n",
    "\n",
    "\n",
    "# For use in demo to border images\n",
    "def add_border(pil_image, border, match):\n",
    "    color = ''\n",
    "    if match:\n",
    "        color = 'green'\n",
    "    else:\n",
    "        color = 'red'\n",
    "        \n",
    "    pil_image = pil_image.convert('RGB')\n",
    "    \n",
    "    if isinstance(border, int) or isinstance(border, tuple):\n",
    "        bimg = ImageOps.expand(pil_image, border=border, fill=color)\n",
    "    else:\n",
    "        raise RuntimeError('Border is not an integer or tuple!')\n",
    "    return bimg\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "\n",
    "\n",
    "# Shuffles a pair of equal size arrays in the same random order\n",
    "# Returns a tuple of shuffled arrays\n",
    "def shuffle_pair(X_arr, y_arr):\n",
    "    X_arr_s = []\n",
    "    y_arr_s = []\n",
    "    for X, y in shuffle(list(zip(X_arr, y_arr))):\n",
    "        X_arr_s.append(X)\n",
    "        y_arr_s.append(y)\n",
    "    return (np.array(X_arr_s), np.array(y_arr_s))\n",
    "\n",
    "\n",
    "# Only for X data, use np.array(y) for y data\n",
    "# Returns data formatted for Keras CNN input\n",
    "def cnn_data(data):\n",
    "    x, y = data.shape[1:]\n",
    "    return data.reshape((-1, x, y, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN hyperparameter tuning with 486 combinations\n",
      "Run 0/486\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 198 samples, validate on 48 samples\n",
      "Epoch 1/1000\n",
      "198/198 [==============================] - 1s 5ms/sample - loss: 0.2784 - accuracy: 0.6566 - val_loss: 0.1875 - val_accuracy: 0.7500\n",
      "Epoch 2/1000\n",
      "198/198 [==============================] - 1s 3ms/sample - loss: 0.1807 - accuracy: 0.7475 - val_loss: 0.1095 - val_accuracy: 0.7500\n",
      "Epoch 3/1000\n",
      "198/198 [==============================] - 1s 3ms/sample - loss: 0.1216 - accuracy: 0.7424 - val_loss: 0.1441 - val_accuracy: 0.7500\n",
      "Epoch 4/1000\n",
      "192/198 [============================>.] - ETA: 0s - loss: 0.1111 - accuracy: 0.7448WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "198/198 [==============================] - 1s 3ms/sample - loss: 0.1123 - accuracy: 0.7424\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8a620cea2e76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-79bb066443fc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;31m# self.train_mlp()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;31m# self.test_best_model()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_cnn_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;31m#         curr_datetime = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-79bb066443fc>\u001b[0m in \u001b[0;36mtrain_cnn_hyperparameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    609\u001b[0m                       \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m                       \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m                       callbacks=[tensorboard, early_stopping])\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf'FINAL-test_acc={model.evaluate(cnn_data(self.test_X),self.test_y)[1]}.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/affective_computing_project/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/affective_computing_project/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m                       \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                       \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m                       total_epochs=1)\n\u001b[0m\u001b[1;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m    397\u001b[0m                                  prefix='val_')\n",
      "\u001b[0;32m~/affective_computing_project/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/affective_computing_project/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/affective_computing_project/env/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/affective_computing_project/env/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/affective_computing_project/env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/affective_computing_project/env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/affective_computing_project/env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/affective_computing_project/env/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/affective_computing_project/env/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    data='../data.pickle'\n",
    "    data_mode='sum'\n",
    "    mode='train'\n",
    "    output_dir='/home/kevin/affective_computing_project/jupyter'\n",
    "    test_files=[]\n",
    "    train_files=[]\n",
    "    val_files=[]\n",
    "    \n",
    "    \n",
    "args = Args()\n",
    "dd = DrunkDetector(args)\n",
    "\n",
    "if args.mode == 'data':\n",
    "    dd.format_data()\n",
    "\n",
    "elif args.mode == 'train':\n",
    "    dd.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
